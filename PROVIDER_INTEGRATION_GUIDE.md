# Provider é…ç½®ç³»ç»Ÿé›†æˆæŒ‡å—

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•é…ç½®å’Œä½¿ç”¨ä¸åŒçš„ LLM Providerï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•ä¸ç³»ç»Ÿå„éƒ¨åˆ†é›†æˆã€‚

## ğŸ“Š ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     å‰ç«¯ UI                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ èŠå¤©ç•Œé¢    â”‚  â”‚ é…ç½®ç®¡ç†      â”‚  â”‚ Agentç®¡ç†     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                  â”‚
         â–¼                 â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      åç«¯ API                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ /api/chat    â”‚  â”‚ /api/config  â”‚  â”‚ WebSocket   â”‚   â”‚
â”‚  â”‚ (èŠå¤©API)     â”‚  â”‚ (é…ç½®ç®¡ç†)    â”‚  â”‚ (AgentLoop) â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ç»„ä»¶ç®¡ç†å™¨ (ComponentManager)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  get_llm_provider()  â† æ ¸å¿ƒæ–¹æ³•                  â”‚   â”‚
â”‚  â”‚                                                   â”‚   â”‚
â”‚  â”‚  1. æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶ Copilot                           â”‚   â”‚
â”‚  â”‚  2. ä½¿ç”¨å·¥å‚åˆ›å»º Provider                          â”‚   â”‚
â”‚  â”‚  3. ç¼“å­˜ Provider å®ä¾‹                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â”‚                                                â”‚
â”‚          â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Provider Factory (providers/factory.py)  â”‚   â”‚
â”‚  â”‚                                                   â”‚   â”‚
â”‚  â”‚  ä¼˜å…ˆçº§æ£€æŸ¥é¡ºåº:                                   â”‚   â”‚
â”‚  â”‚  1. vLLM (æœ¬åœ°)    â† api_base å­˜åœ¨                â”‚   â”‚
â”‚  â”‚  2. ç«å±±å¼•æ“       â† api_key å­˜åœ¨                 â”‚   â”‚
â”‚  â”‚  3. OpenRouter    â† api_key å­˜åœ¨                 â”‚   â”‚
â”‚  â”‚  4. Anthropic     â† api_key å­˜åœ¨                 â”‚   â”‚
â”‚  â”‚  5. OpenAI        â† api_key å­˜åœ¨                 â”‚   â”‚
â”‚  â”‚  6. Groq          â† api_key å­˜åœ¨                 â”‚   â”‚
â”‚  â”‚  7. Gemini        â† api_key å­˜åœ¨                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Provider å®ç°å±‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ GitHubCopilotâ”‚  â”‚ LiteLLM      â”‚                    â”‚
â”‚  â”‚ Provider     â”‚  â”‚ Provider     â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚       â”‚                    â”‚                             â”‚
â”‚       â–¼                    â–¼                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚      Base LLM Provider API         â”‚                 â”‚
â”‚  â”‚  - chat()                          â”‚                 â”‚
â”‚  â”‚  - chat_stream()                   â”‚                 â”‚
â”‚  â”‚  - get_default_model()             â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ é…ç½®æµç¨‹

### 1. ç”¨æˆ·é…ç½® Provider

**æ–¹å¼ A: Web UI**
```
1. è®¿é—® http://localhost:18790
2. è¿›å…¥ã€Œé…ç½®ç®¡ç†ã€
3. é€‰æ‹©ã€ŒLLM Providersã€æ ‡ç­¾
4. é€‰æ‹© Provider ç±»å‹ï¼ˆå¦‚ï¼šæœ¬åœ° OpenAI æ ‡å‡†æ¥å£ï¼‰
5. å¡«å†™é…ç½®:
   - API Base: http://localhost:8000/v1
   - API Key: dummy
   - é»˜è®¤æ¨¡å‹: llama-3-8b
6. ç‚¹å‡»ã€Œä¿å­˜é…ç½®ã€
```

**æ–¹å¼ B: æ‰‹åŠ¨ç¼–è¾‘é…ç½®æ–‡ä»¶**
```json
// ~/.nanobot/config.json
{
  "providers": {
    "vllm": {
      "api_base": "http://localhost:8000/v1",
      "api_key": "dummy"
    }
  },
  "agents": {
    "defaults": {
      "model": "llama-3-8b"
    }
  }
}
```

### 2. ç³»ç»ŸåŠ è½½é…ç½®

å¯åŠ¨æœåŠ¡æ—¶ï¼š
```python
# nanobot/core/dependencies.py
manager = get_component_manager()
config = manager.get_config()  # åŠ è½½ ~/.nanobot/config.json
```

### 3. åˆ›å»º Provider

é¦–æ¬¡è°ƒç”¨ `get_llm_provider()` æ—¶ï¼š
```python
# 1. æ£€æŸ¥ç¼“å­˜
if self._llm_provider is None:
    # 2. åŠ è½½é…ç½®
    config = self.get_config()

    # 3. è°ƒç”¨å·¥å‚åˆ›å»º
    from nanobot.providers.factory import create_llm_provider
    self._llm_provider = create_llm_provider(
        config,
        default_model=config.agents.defaults.model
    )
```

å·¥å‚æ£€æŸ¥é…ç½®ä¼˜å…ˆçº§ï¼š
```python
# nanobot/providers/factory.py
providers_config = config.providers

# ä¼˜å…ˆçº§ 1: æœ¬åœ° vLLM
if providers_config.vllm.api_base:
    return LiteLLMProvider(
        api_key=providers_config.vllm.api_key,
        api_base=providers_config.vllm.api_base,
        default_model=default_model
    )

# ä¼˜å…ˆçº§ 2: ç«å±±å¼•æ“
if providers_config.zhipu.api_key:
    return LiteLLMProvider(...)

# ... å…¶ä»– Provider
```

### 4. ä½¿ç”¨ Provider

**èŠå¤© API** (`/api/chat`):
```python
# nanobot/api/routes/chat.py
provider = manager.get_llm_provider()
result = await provider.chat(
    messages=messages,
    model=request.model,
    ...
)
```

**Agent Loop** (WebSocket èŠå¤©):
```python
# nanobot/core/dependencies.py
async def get_agent_loop(self):
    provider = self.get_llm_provider()
    self._agent_loop = AgentLoop(
        bus=self.get_message_bus(),
        provider=provider,
        workspace=config.workspace_path,
        model=config.agents.defaults.model,
        ...
    )
```

**å­ Agent è°ƒç”¨**:
```python
# nanobot/agent/core/subagent.py
# SubagentManager ä½¿ç”¨ç›¸åŒçš„ Provider
self.subagents = SubagentManager(
    provider=provider,
    workspace=workspace,
    model=self.model,
    ...
)
```

## ğŸ¯ Provider é€‰æ‹©é€»è¾‘

### å½“å‰è¡Œä¸º

```python
def get_llm_provider(self, force_copilot: bool = False):
    """
    è·å– LLM Provider

    ä¼˜å…ˆçº§ï¼ˆé»˜è®¤ force_copilot=Falseï¼‰:
    1. é…ç½®çš„ Provider (vLLM, ç«å±±å¼•æ“ç­‰)
       - å¦‚æœé…ç½®äº†å¤šä¸ªï¼ŒæŒ‰å·¥å‚ä¼˜å…ˆçº§é€‰æ‹©
    2. Copilotï¼ˆå›é€€ï¼‰
       - ä»…å½“æ²¡æœ‰é…ç½®ä»»ä½• Provider ä¸” Copilot å·²è®¤è¯

    ä¼˜å…ˆçº§ï¼ˆforce_copilot=Trueï¼‰:
    1. Copilotï¼ˆå¼ºåˆ¶ï¼‰
       - ä»…å½“ Copilot å·²è®¤è¯
    2. Noneï¼ˆè¿”å› Noneï¼‰
    """
```

### é…ç½®ç¤ºä¾‹

#### åœºæ™¯ 1: ä½¿ç”¨æœ¬åœ° vLLM

```json
{
  "providers": {
    "vllm": {
      "api_base": "http://localhost:8000/v1",
      "api_key": "dummy"
    }
  },
  "agents": {
    "defaults": {
      "model": "llama-3-8b"
    }
  }
}
```

**ç»“æœ**:
- èŠå¤©ç•Œé¢ â†’ ä½¿ç”¨æœ¬åœ°æ¨¡å‹
- Agent Loop â†’ ä½¿ç”¨æœ¬åœ°æ¨¡å‹
- å­ Agent â†’ ä½¿ç”¨æœ¬åœ°æ¨¡å‹

#### åœºæ™¯ 2: ä½¿ç”¨ç«å±±å¼•æ“

```json
{
  "providers": {
    "zhipu": {
      "api_key": "your-zhipu-api-key",
      "api_base": "https://open.bigmodel.cn/api/paas/v4/"
    }
  },
  "agents": {
    "defaults": {
      "model": "glm-4"
    }
  }
}
```

**ç»“æœ**:
- èŠå¤©ç•Œé¢ â†’ ä½¿ç”¨ç«å±±å¼•æ“
- Agent Loop â†’ ä½¿ç”¨ç«å±±å¼•æ“
- å­ Agent â†’ ä½¿ç”¨ç«å±±å¼•æ“

#### åœºæ™¯ 3: åŒæ—¶é…ç½®å¤šä¸ª

```json
{
  "providers": {
    "vllm": {
      "api_base": "http://localhost:8000/v1",
      "api_key": "dummy"
    },
    "openai": {
      "api_key": "sk-..."
    }
  }
}
```

**ç»“æœ**:
- ä¼˜å…ˆä½¿ç”¨ vLLMï¼ˆå·¥å‚ä¼˜å…ˆçº§ 1ï¼‰
- OpenAI è¢«å¿½ç•¥

## ğŸ”§ è°ƒè¯•

### æŸ¥çœ‹å½“å‰ä½¿ç”¨çš„ Provider

```python
# æµ‹è¯•è„šæœ¬
from nanobot.core.dependencies import get_component_manager

manager = get_component_manager()
provider = manager.get_llm_provider()

print(f"Provider ç±»å‹: {type(provider).__name__}")
print(f"é»˜è®¤æ¨¡å‹: {provider.get_default_model()}")
```

### æŸ¥çœ‹æ—¥å¿—

å¯åŠ¨æœåŠ¡æ—¶æŸ¥çœ‹æ—¥å¿—ï¼š
```
[INFO] ä½¿ç”¨ vLLM Provider (æœ¬åœ° OpenAI æ ‡å‡†æ¥å£)
[INFO] ä½¿ç”¨é…ç½®çš„ LLM Provider
```

### æµ‹è¯•è¿æ¥

åœ¨é…ç½®ç®¡ç†ç•Œé¢ç‚¹å‡»ã€Œæµ‹è¯•è¿æ¥ã€æŒ‰é’®ï¼Œæˆ–ä½¿ç”¨ APIï¼š
```bash
curl -X POST http://localhost:8000/api/config/providers/test \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "vllm",
    "config": {
      "api_key": "dummy",
      "api_base": "http://localhost:8000/v1"
    }
  }'
```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **é…ç½®ä¿®æ”¹åéœ€è¦é‡å¯æœåŠ¡**
   - åç«¯åœ¨å¯åŠ¨æ—¶åŠ è½½é…ç½®
   - å‰ç«¯ä¿®æ”¹é…ç½®åï¼Œåç«¯ä¸ä¼šè‡ªåŠ¨é‡æ–°åŠ è½½

2. **Provider å®ä¾‹æ˜¯ç¼“å­˜çš„**
   - é¦–æ¬¡è°ƒç”¨ `get_llm_provider()` åå®ä¾‹è¢«ç¼“å­˜
   - ä¿®æ”¹é…ç½®åéœ€è¦é‡å¯æœåŠ¡ä½¿æ–°é…ç½®ç”Ÿæ•ˆ

3. **æ¨¡å‹åç§°å¿…é¡»åŒ¹é…**
   - å‰ç«¯é€‰æ‹©æ¨¡å‹ â†’ åç«¯ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹å
   - ç¡®ä¿é…ç½®çš„æ¨¡å‹åœ¨ Provider ä¸­å¯ç”¨

4. **Copilot è®¤è¯ä¸ Provider é…ç½®**
   - Copilot è®¤è¯åœ¨ã€Œè´¦å·æ± ç®¡ç†ã€é¡µé¢
   - å…¶ä»– Provider åœ¨ã€Œé…ç½®ç®¡ç† - LLM Providersã€é¡µé¢
   - é…ç½®äº†å…¶ä»– Provider åï¼ŒCopilot ä»å¯ç”¨ä½œå¤‡ç”¨

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. å¯åŠ¨æœ¬åœ° vLLM
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Meta-Llama-3-8B-Instruct \
  --port 8000

# 2. é…ç½® NanoBot
# è®¿é—® http://localhost:18790
# è¿›å…¥ã€Œé…ç½®ç®¡ç†ã€â†’ã€ŒLLM Providersã€
# é€‰æ‹©ã€Œæœ¬åœ° OpenAI æ ‡å‡†æ¥å£ã€
# å¡«å†™ API Base: http://localhost:8000/v1
# ç‚¹å‡»ã€Œä¿å­˜é…ç½®ã€

# 3. é‡å¯ NanoBot
python start.py

# 4. å¼€å§‹èŠå¤©
# èŠå¤©ç•Œé¢ä¼šæ˜¾ç¤ºã€ŒğŸ  æœ¬åœ°æ¥å£ã€å¾½ç« 
# æ¨¡å‹ä¸‹æ‹‰æ¡†ä¼šæ˜¾ç¤ºæœ¬åœ°æ¨¡å‹åˆ—è¡¨
```
