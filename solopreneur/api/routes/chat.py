"""
èŠå¤© API ç«¯ç‚¹
å¤„ç†ç”¨æˆ·ä¸?Agent çš„å¯¹è¯?
"""
from loguru import logger
from typing import Optional

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field, field_validator

from solopreneur.session.cache import get_session_cache

router = APIRouter()


class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚æ¨¡å‹"""
    content: str = Field(
        ...,
        min_length=1,
        max_length=50000,
        description="æ¶ˆæ¯å†…å®¹ï¼Œé•¿åº¦åœ¨1-50000å­—ç¬¦ä¹‹é—´"
    )
    session_id: Optional[str] = Field(
        default="default",
        description="ä¼šè¯IDï¼Œç”¨äºä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡"
    )
    model: Optional[str] = Field(
        default="gpt-5-mini",
        pattern=r"^[a-zA-Z0-9\-_.]+$",
        max_length=100,
        description="æ¨¡å‹åç§°ï¼Œåªå…è®¸å­—æ¯æ•°å­—-_."
    )
    temperature: Optional[float] = Field(
        default=0.7,
        ge=0.0,
        le=2.0,
        description="æ¸©åº¦å‚æ•°ï¼ŒèŒƒå›?.0-2.0"
    )
    max_tokens: Optional[int] = Field(
        default=4096,
        ge=1,
        le=128000,
        description="å•æ¬¡å“åº”æœ€å¤§tokenæ•°ï¼ŒèŒƒå›´1-128000"
    )
    max_iterations: Optional[int] = Field(
        default=None,
        ge=1,
        le=100,
        description="æœ€å¤§å·¥å…·è¿­ä»£æ¬¡æ•°ï¼ˆAgentLoop æ¨¡å¼ä¸‹ï¼‰"
    )
    max_session_tokens: Optional[int] = Field(
        default=None,
        ge=10000,
        le=10000000,
        description="ä¼šè¯ç´¯è®¡æœ€å¤§tokenæ•°ï¼ˆAgentLoop æ¨¡å¼ä¸‹ï¼‰"
    )
    use_tools: Optional[bool] = Field(
        default=False,
        description="æ˜¯å¦å¯ç”¨å·¥å…·è°ƒç”¨ï¼ˆAgentLoop æ¨¡å¼ï¼?
    )
    max_history: Optional[int] = Field(
        default=20,
        ge=1,
        le=100,
        description="ä¿ç•™çš„æœ€å¤§å†å²æ¶ˆæ¯æ•°"
    )
    clear_history: Optional[bool] = Field(
        default=False,
        description="æ˜¯å¦æ¸…ç©ºä¼šè¯å†å²åå¼€å§‹å¯¹è¯?
    )

    @field_validator('content')
    @classmethod
    def validate_content(cls, v: str) -> str:
        """éªŒè¯æ¶ˆæ¯å†…å®¹"""
        # å»é™¤é¦–å°¾ç©ºç™½
        v = v.strip()
        if not v:
            raise ValueError("æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º")

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šé‡å¤å­—ç¬¦ï¼ˆé˜²æ­¢åƒåœ¾å†…å®¹ï¼?
        if len(set(v)) < 3 and len(v) > 10:
            raise ValueError("æ¶ˆæ¯å†…å®¹æ— æ•ˆï¼šåŒ…å«è¿‡å¤šé‡å¤å­—ç¬?)

        return v


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ¨¡å‹"""
    response: str
    session_id: str


# é»˜è®¤ç³»ç»Ÿæç¤ºè¯?
DEFAULT_SYSTEM_PROMPT = "ä½ æ˜¯ NanoBotï¼Œä¸€ä¸ªå‹å¥½ã€æ™ºèƒ½çš„ AI åŠ©æ‰‹ã€‚ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€ç¼–å†™ä»£ç ã€åˆ†æé—®é¢˜ç­‰ã€‚è¯·ç”¨ä¸­æ–‡å›å¤ã€?


@router.post("/chat", response_model=ChatResponse)
async def send_message(request: ChatRequest):
    """
    å‘é€æ¶ˆæ¯ç»™ Agent

    Args:
        request: åŒ…å«æ¶ˆæ¯å†…å®¹çš„è¯·æ±?

    Returns:
        ChatResponse: Agent çš„å›å¤?
    """
    logger.info(f"Received chat message: {request.content[:50]}... (session: {request.session_id})")

    try:
        # ä½¿ç”¨ç»„ä»¶ç®¡ç†å™¨è·å–é…ç½®çš„ Provider
        from solopreneur.core.dependencies import get_component_manager
        manager = get_component_manager()
        provider = manager.get_llm_provider()

        # æ£€æŸ?Provider æ˜¯å¦å¯ç”¨
        if provider is None:
            logger.warning("No LLM provider configured, returning config prompt")
            return ChatResponse(
                response="âš ï¸ è¯·å…ˆåœ¨ã€Œé…ç½®ã€é¡µé¢é…ç½?LLM Provider åå†ä½¿ç”¨èŠå¤©åŠŸèƒ½ã€‚\n\n"
                         "1. ç‚¹å‡»å·¦ä¾§èœå•ã€Œé…ç½®ã€\n"
                         "2. åœ¨ã€ŒLLM Providersã€åŒºåŸŸé€‰æ‹©å¹¶é…ç½®ä¸€ä¸?Provider\n"
                         "   - GitHub Copilotï¼ˆè´¦å·æ± ç®¡ç†é¡µé¢ç™»å½•ï¼‰\n"
                         "   - æœ¬åœ° OpenAI æ ‡å‡†æ¥å£ï¼ˆvLLM, Ollama ç­‰ï¼‰\n"
                         "   - ç«å±±å¼•æ“\n"
                         "   - å…¶ä»– Providerï¼ˆOpenAI, Anthropic ç­‰ï¼‰\n"
                         "3. ç‚¹å‡»ã€Œä¿å­˜é…ç½®ã€?,
                session_id=request.session_id
            )

        # è·å–æˆ–åˆ›å»ºä¼šè¯?
        session_cache = get_session_cache()
        session = session_cache.get_or_create(
            session_id=request.session_id,
            system_prompt=DEFAULT_SYSTEM_PROMPT
        )

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        session.add_message("user", request.content)

        # è°ƒç”¨ LLM Provider
        logger.debug(f"Calling LLM Provider with model: {request.model}")
        logger.debug(f"Message history length: {len(session.messages)}")
        logger.debug(f"Current message: {request.content[:100]}...")

        try:
            result = await provider.chat(
                messages=session.to_messages(),
                model=request.model,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                stream=False
            )
        except Exception as api_error:
            logger.error(f"LLM API error: {api_error}")

            # è¾“å‡ºå¯è¯»çš„æ¶ˆæ¯å†…å®¹ï¼ˆç”¨äºè°ƒè¯•ç¼–ç é—®é¢˜ï¼?
            for i, msg in enumerate(session.to_messages()):
                logger.error(f"Message {i} ({msg['role']}): {repr(msg['content'][:100])}...")

            raise

        # æå–å“åº”å†…å®¹ï¼ˆç»Ÿä¸€å¤„ç†ä¸åŒ Provider çš„è¿”å›æ ¼å¼ï¼‰
        if hasattr(result, 'content'):
            # LiteLLM å’Œå…¶ä»–ä½¿ç”?LLMResponse çš?Provider
            response_text = result.content or "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€?
        elif isinstance(result, dict):
            # æ—§çš„ Copilot è¿”å›æ ¼å¼ï¼ˆå…¼å®¹ï¼‰
            choices = result.get("choices", [])
            if choices:
                response_text = choices[0].get("message", {}).get("content", "")
            else:
                response_text = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€?
        elif isinstance(result, str):
            # ç›´æ¥è¿”å›å­—ç¬¦ä¸?
            response_text = result
        else:
            response_text = "æŠ±æ­‰ï¼Œå“åº”æ ¼å¼å¼‚å¸¸ã€?

        # ä¿å­˜åŠ©æ‰‹å›å¤åˆ°å†å?
        session.add_message("assistant", response_text)

        # é™åˆ¶å†å²é•¿åº¦ï¼ˆä¿ç•™æœ€è¿?20 æ¡æ¶ˆæ¯ï¼‰
        session.truncate(20)

        logger.info(f"Chat response generated successfully")
        return ChatResponse(response=response_text, session_id=request.session_id)

    except Exception as e:
        logger.error(f"Chat processing failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/chat/history")
async def clear_chat_history(session_id: Optional[str] = "default"):
    """æ¸…ç©ºå¯¹è¯å†å²"""
    session_cache = get_session_cache()

    if session_id == "all":
        session_cache.clear()
        logger.info("All chat histories cleared")
        return {"status": "ok", "message": "æ‰€æœ‰å¯¹è¯å†å²å·²æ¸…ç©º"}
    else:
        deleted = session_cache.delete(session_id)
        if deleted:
            logger.info(f"Chat history cleared for session: {session_id}")
            return {"status": "ok", message: f"ä¼šè¯ {session_id} çš„å¯¹è¯å†å²å·²æ¸…ç©º"}
        else:
            return {"status": "ok", message: f"ä¼šè¯ {session_id} ä¸å­˜åœ?}


@router.get("/chat/sessions")
async def list_sessions():
    """åˆ—å‡ºæ‰€æœ‰ä¼šè¯?""
    session_cache = get_session_cache()
    sessions = [
        {
            "session_id": s.session_id,
            "message_count": len(s.messages),
            "created_at": s.created_at.isoformat(),
            "updated_at": s.updated_at.isoformat(),
        }
        for s in session_cache.list_sessions()
    ]
    return {"sessions": sessions, "total": len(sessions)}
