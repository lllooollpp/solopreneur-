"""Agent å¾ªç¯ï¼šæ ¸å¿ƒå¤„ç†å¼•æ“ã€?""

import asyncio
import json
import re
import time
from pathlib import Path
from typing import TYPE_CHECKING, Any

from loguru import logger

from solopreneur.bus.events import InboundMessage, OutboundMessage
from solopreneur.bus.queue import MessageBus
from solopreneur.providers.base import LLMProvider
from solopreneur.agent.core.context import ContextBuilder
from solopreneur.agent.core.compaction import CompactionEngine
from solopreneur.agent.core.tools.registry import ToolRegistry
from solopreneur.agent.core.tools.filesystem import ReadFileTool, WriteFileTool, EditFileTool, ListDirTool
from solopreneur.agent.core.tools.db import DBInspectTool
from solopreneur.agent.core.tools.metrics import MetricsInspectTool
from solopreneur.agent.core.tools.repo import GitInspectTool, SearchCodeTool, GitCommandTool
from solopreneur.agent.core.tools.shell import ExecTool
from solopreneur.agent.core.tools.web import WebSearchTool, WebFetchTool
from solopreneur.agent.core.tools.message import MessageTool
from solopreneur.agent.core.tools.spawn import SpawnTool
from solopreneur.agent.core.tools.harness import HarnessTool
from solopreneur.agent.core.tools.project_env import GetProjectEnvTool, SetProjectEnvTool
from solopreneur.agent.core.subagent import SubagentManager
from solopreneur.agent.core.validator import TaskCompletionValidator, ValidatorConfig
from solopreneur.storage import UsagePersistence
from solopreneur.session.manager import SessionManager

if TYPE_CHECKING:
    from solopreneur.agent.core.harness import LongRunningHarness
    from solopreneur.config.schema import ExecToolConfig

# å®‰å…¨é™åˆ¶å¸¸é‡ï¼ˆé»˜è®¤å€¼ï¼Œå¯é€šè¿‡ config è¦†ç›–ï¼?
DEFAULT_MAX_TOTAL_TIME = 1800  # 30åˆ†é’Ÿæ€»æ—¶é—´é™åˆ?
DEFAULT_MAX_TOKENS_PER_SESSION = 500000  # æ¯æ¬¡ä¼šè¯æœ€å¤§Tokenæ•?
# è‡ªåŠ¨å‹ç¼©æœ€å¤§æ¬¡æ•?
MAX_COMPACTION_ROUNDS = 10
# LLM è°ƒç”¨é‡è¯•é…ç½®
LLM_MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•?
LLM_RETRY_BASE_DELAY = 2.0  # åŸºç¡€é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼?
LLM_RETRY_MAX_DELAY = 30.0  # æœ€å¤§é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰


class AgentLoop:
    """
    Agent å¾ªç¯æ˜¯æ ¸å¿ƒå¤„ç†å¼•æ“ã€?
    
    å®ƒè´Ÿè´£ï¼š
    1. ä»æ€»çº¿æ¥æ”¶æ¶ˆæ¯
    2. ä½¿ç”¨å†å²è®°å½•ã€è®°å¿†å’ŒæŠ€èƒ½æ„å»ºä¸Šä¸‹æ–‡
    3. è°ƒç”¨ LLM
    4. æ‰§è¡Œå·¥å…·è°ƒç”¨
    5. å‘å›å“åº”
    """
    
    def __init__(
        self,
        bus: MessageBus,
        provider: LLMProvider,
        workspace: Path,
        model: str | None = None,
        max_iterations: int = 50,
        brave_api_key: str | None = None,
        exec_config: "ExecToolConfig | None" = None,
        max_session_tokens: int = 0,
        max_total_time: int = 0,
        validator_config: "ValidatorConfig | None" = None,
    ):
        from solopreneur.config.schema import ExecToolConfig
        self.bus = bus
        self.provider = provider
        self.workspace = workspace
        self.model = model or provider.get_default_model()
        self.max_iterations = max_iterations
        self.brave_api_key = brave_api_key
        self.exec_config = exec_config or ExecToolConfig()
        self.max_session_tokens = max_session_tokens or DEFAULT_MAX_TOKENS_PER_SESSION
        self.max_total_time = max_total_time or DEFAULT_MAX_TOTAL_TIME
        
        self.context = ContextBuilder(workspace)
        self.usage_store = UsagePersistence()
        self.sessions = SessionManager(workspace)
        self.tools = ToolRegistry()
        self.subagents = SubagentManager(
            provider=provider,
            workspace=workspace,
            bus=bus,
            model=self.model,
            brave_api_key=brave_api_key,
            exec_config=self.exec_config,
        )
        
        # ä»»åŠ¡å®ŒæˆéªŒè¯å™?
        self.validator_config = validator_config or ValidatorConfig()
        self.validator = TaskCompletionValidator(
            workspace=workspace,
            harness=None,  # ç¨åé€šè¿‡ set_harness è®¾ç½®
            config=self.validator_config,
            provider=provider,  # ä¼ é€?provider ç”¨äº AI éªŒè¯
            model=self.model,
        )

        # ä¸‰å±‚å‹ç¼©å¼•æ“
        self.compaction = CompactionEngine(
            provider=provider,
            workspace=workspace,
            model=self.model,
        )

        # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ?
        from solopreneur.agent.definitions.manager import AgentManager
        from solopreneur.workflow.engine import WorkflowEngine

        # ç›´æ¥åˆ›å»º AgentManagerï¼ˆç‹¬ç«‹å®ä¾‹ï¼Œä¸ä½¿ç”¨ç»„ä»¶ç®¡ç†å™¨ï¼?
        # è¿™å…è®¸æ¯ä¸?AgentLoop æœ‰è‡ªå·±çš„ AgentManager
        self.agent_manager = AgentManager(
            workspace=workspace,
            skills_loader=self.context.skills,
        )
        self.workflow_engine = WorkflowEngine(
            subagent_manager=self.subagents,
            agent_manager=self.agent_manager,
            workspace=workspace,
        )
        
        self._running = False
        self._register_default_tools()

    def set_harness(self, harness: "LongRunningHarness | None") -> None:
        """
        è®¾ç½® LongRunningHarness å®ä¾‹
        
        ç”¨äºä»»åŠ¡å®ŒæˆéªŒè¯å™¨æ£€æŸ?feature_list çŠ¶æ€?
        åŒæ—¶ä¼ é€’ç»™ WorkflowEngine ä»¥å¯ç”?effc.md å¢é‡æ¨¡å¼
        """
        self.validator.harness = harness
        self.workflow_engine.set_harness(harness)
        logger.debug(f"Validator & WorkflowEngine harness set: {harness is not None}")

    def _record_usage(
        self,
        session_key: str,
        model: str,
        usage: dict[str, int] | None,
        duration_ms: int,
        is_stream: bool,
    ) -> None:
        """è®°å½•ä¸€æ¬?LLM è°ƒç”¨ usageï¼ˆå¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼‰ã€?""
        usage = usage or {}
        try:
            self.usage_store.record(
                session_key=session_key,
                model=model,
                prompt_tokens=usage.get("prompt_tokens", 0),
                completion_tokens=usage.get("completion_tokens", 0),
                total_tokens=usage.get("total_tokens", 0),
                duration_ms=duration_ms,
                is_stream=is_stream,
            )
        except Exception as e:
            logger.warning(f"è®°å½• LLM usage å¤±è´¥: {e}")
    
    async def _call_llm_with_retry(
        self,
        messages: list[dict],
        tools: list[dict] | None,
        on_chunk: Any = None,
        use_stream: bool = True,
    ) -> tuple[Any, bool]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„ LLM è°ƒç”¨ã€?

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            on_chunk: æµå¼å›è°ƒ
            use_stream: æ˜¯å¦ä½¿ç”¨æµå¼è°ƒç”¨

        Returns:
            (response, success) - å“åº”å¯¹è±¡å’Œæ˜¯å¦æˆåŠ?

        Raises:
            æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥åæŠ›å‡ºå¼‚å¸¸
        """
        import random
        from httpx import ReadTimeout, ConnectTimeout, ReadError

        last_error = None

        for attempt in range(LLM_MAX_RETRIES):
            try:
                if use_stream and hasattr(self.provider, "chat_stream") and on_chunk:
                    response = await self.provider.chat_stream(
                        messages=messages,
                        tools=tools,
                        model=self.model,
                        on_chunk=on_chunk,
                    )
                else:
                    response = await self.provider.chat(
                        messages=messages,
                        tools=tools,
                        model=self.model,
                    )
                return response, True

            except (ReadTimeout, ConnectTimeout, ReadError, asyncio.TimeoutError) as e:
                last_error = e
                if attempt < LLM_MAX_RETRIES - 1:
                    # æŒ‡æ•°é€€é?+ éšæœºæŠ–åŠ¨
                    delay = min(
                        LLM_RETRY_BASE_DELAY * (2 ** attempt) + random.uniform(0, 1),
                        LLM_RETRY_MAX_DELAY
                    )
                    logger.warning(
                        f"LLM è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{LLM_MAX_RETRIES}): {type(e).__name__}: {e}ã€?
                        f"å°†åœ¨ {delay:.1f} ç§’åé‡è¯•..."
                    )
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"LLM è°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•?({LLM_MAX_RETRIES})")
                    raise

            except Exception as e:
                # å…¶ä»–å¼‚å¸¸ç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
                logger.error(f"LLM è°ƒç”¨é‡åˆ°éç½‘ç»œé”™è¯? {type(e).__name__}: {e}")
                raise

        # ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œï¼Œä½†ä¸ºäº†ç±»å‹å®‰å…?
        if last_error:
            raise last_error
        raise RuntimeError("LLM è°ƒç”¨å¤±è´¥ï¼ŒæœªçŸ¥é”™è¯?)

    def _register_default_tools(self) -> None:
        """æ³¨å†Œé»˜è®¤å·¥å…·é›†ã€?""
        # æ–‡ä»¶å·¥å…·ï¼ˆå¸¦å·¥ä½œç©ºé—´é™åˆ¶ï¼?
        self.tools.register(ReadFileTool(workspace=self.workspace))
        self.tools.register(WriteFileTool(workspace=self.workspace))
        self.tools.register(EditFileTool(workspace=self.workspace))
        self.tools.register(ListDirTool(workspace=self.workspace))
        self.tools.register(DBInspectTool())
        self.tools.register(MetricsInspectTool())
        self.tools.register(SearchCodeTool(workspace=self.workspace))
        self.tools.register(GitInspectTool(workspace=self.workspace))
        self.tools.register(GitCommandTool(workspace=self.workspace))

        # Harness å·¥å…·ï¼ˆé•¿æœŸä»»åŠ¡ç®¡ç†ï¼‰
        self.tools.register(HarnessTool(workspace=self.workspace))
        self.tools.register(GetProjectEnvTool(workspace=self.workspace))
        self.tools.register(SetProjectEnvTool(workspace=self.workspace))

        # Shell å·¥å…·
        self.tools.register(ExecTool(
            working_dir=str(self.workspace),
            timeout=self.exec_config.timeout,
            restrict_to_workspace=self.exec_config.restrict_to_workspace,
            whitelist_mode=self.exec_config.whitelist_mode,
        ))
        
        # Web å·¥å…·
        self.tools.register(WebSearchTool(api_key=self.brave_api_key))
        self.tools.register(WebFetchTool())
        
        # æ¶ˆæ¯å·¥å…·
        message_tool = MessageTool(send_callback=self.bus.publish_outbound)
        self.tools.register(message_tool)
        
        # è¡ç”Ÿè¾…åŠ©å·¥å…· (Spawn toolï¼Œç”¨äºå­ agent)
        spawn_tool = SpawnTool(manager=self.subagents)
        self.tools.register(spawn_tool)
        
        # è§’è‰²å§”æ´¾å·¥å…· (delegateï¼Œç”¨äºå°†ä»»åŠ¡å§”æ´¾ç»™è½¯ä»¶å·¥ç¨‹è§’è‰?
        from solopreneur.agent.core.tools.delegate import DelegateTool
        from solopreneur.agent.core.tools.delegate_auto import DelegateAutoTool
        from solopreneur.agent.core.tools.delegate_parallel import DelegateParallelTool
        delegate_tool = DelegateTool(
            manager=self.subagents,
            agent_manager=self.agent_manager,
        )
        self.tools.register(delegate_tool)

        delegate_parallel_tool = DelegateParallelTool(
            manager=self.subagents,
            agent_manager=self.agent_manager,
        )
        self.tools.register(delegate_parallel_tool)

        delegate_auto_tool = DelegateAutoTool(
            manager=self.subagents,
            agent_manager=self.agent_manager,
        )
        self.tools.register(delegate_auto_tool)
        
        # å·¥ä½œæµå·¥å…?(run_workflowï¼Œç”¨äºæ‰§è¡Œé¢„å®šä¹‰çš„å¼€å‘æµæ°´çº¿)
        from solopreneur.workflow.engine import RunWorkflowTool, WorkflowControlTool
        workflow_tool = RunWorkflowTool(engine=self.workflow_engine)
        self.tools.register(workflow_tool)
        
        control_tool = WorkflowControlTool(engine=self.workflow_engine)
        self.tools.register(control_tool)

    def _resolve_request_workspace(self, project_info: dict | None) -> Path:
        """æ ¹æ®é¡¹ç›®ä¸Šä¸‹æ–‡è§£ææœ¬æ¬¡è¯·æ±‚åº”ä½¿ç”¨çš„å·¥ä½œç›®å½•ã€?""
        if project_info and project_info.get("path"):
            try:
                p = Path(str(project_info.get("path"))).expanduser().resolve()
                if p.exists() and p.is_dir():
                    return p
                logger.warning(f"é¡¹ç›®è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•ï¼Œå›é€€åˆ°é»˜è®¤å·¥ä½œåŒº: {p}")
            except Exception as e:
                logger.warning(f"è§£æé¡¹ç›®è·¯å¾„å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤å·¥ä½œåŒº: {e}")
        return self.workspace

    def _build_request_tools(self, request_workspace: Path) -> ToolRegistry:
        """ä¸ºå•æ¬¡è¯·æ±‚æ„å»ºå·¥å…·æ³¨å†Œè¡¨ï¼Œç¡®ä¿æ–‡ä»?å‘½ä»¤ä½œç”¨åŸŸæ­£ç¡®ã€?""
        tools = ToolRegistry()

        # è·¯å¾„æ•æ„Ÿå·¥å…·ï¼šç»‘å®šåˆ°å½“å‰è¯·æ±‚å·¥ä½œç›®å½•
        tools.register(ReadFileTool(workspace=request_workspace))
        tools.register(WriteFileTool(workspace=request_workspace))
        tools.register(EditFileTool(workspace=request_workspace))
        tools.register(ListDirTool(workspace=request_workspace))
        tools.register(SearchCodeTool(workspace=request_workspace))
        tools.register(GitInspectTool(workspace=request_workspace))
        tools.register(GitCommandTool(workspace=request_workspace))
        tools.register(HarnessTool(workspace=request_workspace))
        tools.register(GetProjectEnvTool(workspace=request_workspace))
        tools.register(SetProjectEnvTool(workspace=request_workspace))
        tools.register(ExecTool(
            working_dir=str(request_workspace),
            timeout=self.exec_config.timeout,
            restrict_to_workspace=self.exec_config.restrict_to_workspace,
            whitelist_mode=self.exec_config.whitelist_mode,
        ))

        # éè·¯å¾„æ•æ„?çŠ¶æ€å·¥å…·ï¼šæ²¿ç”¨å·²æ³¨å†Œå®ä¾?
        for name in [
            "db_inspect", "metrics_inspect", "web_search", "web_fetch",
            "message", "spawn", "delegate", "delegate_parallel", "delegate_auto",
            "run_workflow", "workflow_control",
        ]:
            t = self.tools.get(name)
            if t is not None:
                tools.register(t)

        return tools

    # â”€â”€ ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆå§”æ‰˜ç»?CompactionEngineï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _compact_context(self, messages: list[dict]) -> list[dict]:
        """
        ä½¿ç”¨ä¸‰å±‚å‹ç¼©å¼•æ“å‹ç¼©ä¸Šä¸‹æ–‡ã€?

        1. å…ˆæ‰§è¡Œå¾®å‹ç¼©ï¼ˆå¤§å‹å·¥å…·è¾“å‡ºè½ç›˜ï¼‰
        2. å†æ‰§è¡?LLM é©±åŠ¨çš„è‡ªåŠ¨å‹ç¼©ï¼ˆç»“æ„åŒ–æ‘˜è¦ï¼‰

        Returns:
            å‹ç¼©åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        # å±?: å¾®å‹ç¼?
        messages = self.compaction.microcompact(messages)

        # å±?: LLM æ‘˜è¦å‹ç¼©
        messages = await self.compaction.auto_compact(messages)

        return messages

    async def run(self) -> None:
        """è¿è¡Œ agent å¾ªç¯ï¼Œå¤„ç†æ€»çº¿ä¼ æ¥çš„æ¶ˆæ¯ã€?""
        self._running = True
        logger.info("Agent å¾ªç¯å·²å¯åŠ?)
        
        while self._running:
            try:
                # ç­‰å¾…ä¸‹ä¸€æ¡æ¶ˆæ?
                msg = await asyncio.wait_for(
                    self.bus.consume_inbound(),
                    timeout=1.0
                )
                
                # å¤„ç†æ¶ˆæ¯
                try:
                    response = await self._process_message(msg)
                    if response:
                        await self.bus.publish_outbound(response)
                except Exception as e:
                    logger.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”? {e}")
                    # å‘é€é”™è¯¯å“åº?
                    await self.bus.publish_outbound(OutboundMessage(
                        channel=msg.channel,
                        chat_id=msg.chat_id,
                        content=f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€ä¸ªé”™è¯? {str(e)}"
                    ))
            except asyncio.TimeoutError:
                continue
    
    def stop(self) -> None:
        """åœæ­¢ agent å¾ªç¯ã€?""
        self._running = False
        logger.info("Agent å¾ªç¯æ­£åœ¨åœæ­¢")
    
    async def _process_message(self, msg: InboundMessage) -> OutboundMessage | None:
        """
        å¤„ç†å•æ¡å…¥ç«™æ¶ˆæ¯ã€?
        
        å‚æ•°:
            msg: è¦å¤„ç†çš„å…¥ç«™æ¶ˆæ¯ã€?
        
        è¿”å›:
            å“åº”æ¶ˆæ¯ï¼Œå¦‚æœä¸éœ€è¦å“åº”åˆ™ä¸?Noneã€?
        """
        # å¤„ç†ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå­ agent å®£å‘Šï¼?
        # chat_id åŒ…å«åŸå§‹çš?"channel:chat_id"ï¼Œä»¥ä¾¿è·¯ç”±å›åŸå¤„
        if msg.channel == "system":
            return await self._process_system_message(msg)
        
        logger.info(f"æ­£åœ¨å¤„ç†æ¥è‡ª {msg.channel}:{msg.sender_id} çš„æ¶ˆæ?)
        
        # è·å–æˆ–åˆ›å»ºä¼šè¯?
        session = self.sessions.get_or_create(msg.session_key)
        
        # é‡ç½®éªŒè¯å™¨çŠ¶æ€ï¼ˆæ–°ä¼šè¯å¼€å§‹ï¼‰
        self.validator.reset()
        # è®¾ç½®éªŒè¯ä¸Šä¸‹æ–‡ï¼ˆç”¨æˆ·è¯·æ±‚ï¼?
        self.validator.set_context(user_request=msg.content)
        
        # æ›´æ–°å·¥å…·ä¸Šä¸‹æ–?
        message_tool = self.tools.get("message")
        if isinstance(message_tool, MessageTool):
            message_tool.set_context(msg.channel, msg.chat_id)
        
        spawn_tool = self.tools.get("spawn")
        if isinstance(spawn_tool, SpawnTool):
            spawn_tool.set_context(msg.channel, msg.chat_id)
        
        # æ„å»ºåˆå§‹æ¶ˆæ¯ï¼ˆä½¿ç”?get_history è·å– LLM æ ¼å¼çš„æ¶ˆæ¯ï¼‰
        messages = self.context.build_messages(
            history=session.get_history(),
            current_message=msg.content,
            media=msg.media if msg.media else None,
        )
        
        # Agent å¾ªç¯ - æ·»åŠ å®‰å…¨é™åˆ¶
        iteration = 0
        final_content = None
        start_time = time.time()
        total_tokens = 0
        compacted_count = 0  # å·²å‹ç¼©æ¬¡æ•?
        
        while iteration < self.max_iterations:
            iteration += 1
            
            # æ—¶é—´æ£€æŸ?
            elapsed_time = time.time() - start_time
            if elapsed_time > self.max_total_time:
                final_content = f"å¤„ç†è¶…æ—¶ï¼ˆè¶…è¿‡{self.max_total_time // 60}åˆ†é’Ÿï¼‰ï¼Œè¯·ç®€åŒ–æ‚¨çš„è¯·æ±‚æˆ–å¼€å§‹æ–°çš„å¯¹è¯ã€?
                logger.warning(f"Agentå¾ªç¯è¶…æ—¶: {elapsed_time:.1f}ç§?)
                break
            
            # è°ƒç”¨ LLM å‰ï¼šä¸»åŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦å‹ç¼?(78% é˜ˆå€?
            if self.compaction.should_compact(messages, self.max_session_tokens):
                compacted_count += 1
                if compacted_count <= MAX_COMPACTION_ROUNDS:
                    logger.info(
                        f"ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡è¾¾åˆ°é˜ˆå€¼ï¼Œæ‰§è¡Œé¢„é˜²æ€§å‹ç¼?(ç¬¬{compacted_count}æ¬?"
                    )
                    messages = await self._compact_context(messages)
                    total_tokens = 0

            # è°ƒç”¨ LLM
            llm_start = time.time()
            response = await self.provider.chat(
                messages=messages,
                tools=self.tools.get_definitions(),
                model=self.model
            )
            llm_duration_ms = int((time.time() - llm_start) * 1000)
            self._record_usage(
                session_key=msg.session_key,
                model=self.model,
                usage=response.usage,
                duration_ms=llm_duration_ms,
                is_stream=False,
            )
            
            # Token æ¶ˆè€—æ£€æŸ¥ï¼šè¶…é™æ—¶å‹ç¼©ä¸Šä¸‹æ–‡è€Œéåœæ­¢
            total_tokens += response.usage.get("total_tokens", 0)
            if total_tokens > self.max_session_tokens:
                compacted_count += 1
                if compacted_count >= MAX_COMPACTION_ROUNDS:
                    # å¤šæ¬¡å‹ç¼©ä»è¶…æ ‡ï¼Œå¼ºåˆ¶åœæ­¢
                    final_content = f"Tokenæ¶ˆè€—è¿‡å¤šï¼ˆ{total_tokens}ï¼‰ï¼Œå·²å¤šæ¬¡å‹ç¼©ä¸Šä¸‹æ–‡ä»æ— æ³•æ§åˆ¶ï¼Œè¯·å¼€å§‹æ–°çš„å¯¹è¯ã€?
                    logger.warning(f"Tokenæ¶ˆè€—è¶…é™ä¸”å‹ç¼©æ¬¡æ•°ç”¨å°½: {total_tokens}")
                    break
                logger.info(
                    f"Tokenç´¯è®¡ {total_tokens}/{self.max_session_tokens} è¶…é™ï¼?
                    f"æ‰§è¡Œä¸Šä¸‹æ–‡å‹ç¼?(ç¬¬{compacted_count}æ¬?"
                )
                messages = await self._compact_context(messages)
                total_tokens = 0  # å‹ç¼©åé‡ç½®è®¡æ•?
            
            # å¤„ç†å·¥å…·è°ƒç”¨
            if response.has_tool_calls:
                # æ·»åŠ å¸¦æœ‰å·¥å…·è°ƒç”¨çš?assistant æ¶ˆæ¯
                tool_call_dicts = [
                    {
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.name,
                            "arguments": json.dumps(tc.arguments)  # å¿…é¡»æ˜?JSON å­—ç¬¦ä¸?
                        }
                    }
                    for tc in response.tool_calls
                ]
                messages = self.context.add_assistant_message(
                    messages, response.content, tool_call_dicts
                )
                
                # æ‰§è¡Œå·¥å…·
                for tool_call in response.tool_calls:
                    args_str = json.dumps(tool_call.arguments)
                    logger.debug(f"æ­£åœ¨æ‰§è¡Œå·¥å…·ï¼š{tool_call.name}ï¼Œå‚æ•°ï¼š{args_str}")
                    result = await self.tools.execute(tool_call.name, tool_call.arguments)
                    messages = self.context.add_tool_result(
                        messages, tool_call.id, tool_call.name, result
                    )

                # å·¥å…·è°ƒç”¨åæ‰§è¡Œå¾®å‹ç¼©ï¼ˆå¤§å‹è¾“å‡ºè½ç›˜ï¼‰
                messages = self.compaction.microcompact(messages)
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼ŒéªŒè¯ä»»åŠ¡æ˜¯å¦çœŸæ­£å®Œæˆ?
                should_continue = False
                continuation_prompt = ""
                
                # 1. æ£€æŸ¥æœ€å°è¿­ä»£æ¬¡æ•?
                force_continue, reason = self.validator.should_force_continue(iteration)
                if force_continue:
                    should_continue = True
                    continuation_prompt = f"ä»»åŠ¡åˆšå¼€å§‹ï¼Œè¯·ç»§ç»­è°ƒç”¨å·¥å…·å·¥ä½œã€‚\nåŸå› : {reason}"
                    logger.info(f"è¿­ä»£æ¬¡æ•°ä¸è¶³ ({iteration}/{self.validator_config.min_iterations})ï¼Œå¼ºåˆ¶ç»§ç»?)
                
                # 2. ä»»åŠ¡å®ŒæˆéªŒè¯ï¼ˆä»…å½“æœ€å°è¿­ä»£æ»¡è¶³æ—¶ï¼?
                if not should_continue and self.validator.can_send_continuation_prompt():
                    validation_result = await self.validator.validate(response.content or "")
                    if not validation_result.is_complete:
                        should_continue = True
                        continuation_prompt = validation_result.get_continuation_prompt()
                        self.validator.increment_continuation_count()
                        logger.info(f"ä»»åŠ¡å®ŒæˆéªŒè¯æœªé€šè¿‡: {validation_result.reasons}")
                
                if should_continue:
                    # æ³¨å…¥ç»§ç»­æç¤ºï¼Œå¼ºåˆ?LLM ç»§ç»­å·¥ä½œ
                    messages.append({
                        "role": "user",
                        "content": continuation_prompt
                    })
                    continue
                
                # éªŒè¯é€šè¿‡ï¼Œå¤„ç†å®Œæˆ?
                final_content = response.content
                break
        
        if final_content is None:
            final_content = "æˆ‘å·²å¤„ç†å®Œæ¯•ï¼Œä½†æ²¡æœ‰ç”Ÿæˆä»»ä½•å“åº”ã€?
        
        # ä¿å­˜åˆ°ä¼šè¯?
        session.add_message("user", msg.content)
        session.add_message("assistant", final_content)
        self.sessions.save(session)
        
        return OutboundMessage(
            channel=msg.channel,
            chat_id=msg.chat_id,
            content=final_content
        )
    
    async def _process_system_message(self, msg: InboundMessage) -> OutboundMessage | None:
        """
        å¤„ç†ç³»ç»Ÿæ¶ˆæ¯ï¼ˆä¾‹å¦‚å­ agent å®£å‘Šï¼‰ã€?
        
        chat_id å­—æ®µåŒ…å« "original_channel:original_chat_id"ï¼Œä»¥ä¾¿å°†å“åº”è·¯ç”±å›æ­£ç¡®çš„ç›®çš„åœ°ã€?
        """
        logger.info(f"æ­£åœ¨å¤„ç†æ¥è‡ª {msg.sender_id} çš„ç³»ç»Ÿæ¶ˆæ?)
        
        # ä»?chat_id è§£ææ¥æºï¼ˆæ ¼å¼ï¼š"channel:chat_id"ï¼?
        if ":" in msg.chat_id:
            parts = msg.chat_id.split(":", 1)
            origin_channel = parts[0]
            origin_chat_id = parts[1]
        else:
            # å¤‡é€‰æ–¹æ¡?
            origin_channel = "cli"
            origin_chat_id = msg.chat_id
        
        # ä½¿ç”¨åŸå§‹ä¼šè¯ä½œä¸ºä¸Šä¸‹æ–?
        session_key = f"{origin_channel}:{origin_chat_id}"
        session = self.sessions.get_or_create(session_key)
        
        # æ›´æ–°å·¥å…·ä¸Šä¸‹æ–?
        message_tool = self.tools.get("message")
        if isinstance(message_tool, MessageTool):
            message_tool.set_context(origin_channel, origin_chat_id)
        
        spawn_tool = self.tools.get("spawn")
        if isinstance(spawn_tool, SpawnTool):
            spawn_tool.set_context(origin_channel, origin_chat_id)
        
        # ä½¿ç”¨å®£å‘Šå†…å®¹æ„å»ºæ¶ˆæ¯
        messages = self.context.build_messages(
            history=session.get_history(),
            current_message=msg.content
        )
        
        # Agent å¾ªç¯ï¼ˆé’ˆå¯¹å®£å‘Šå¤„ç†è¿›è¡Œäº†é™åˆ¶ï¼?
        iteration = 0
        final_content = None
        
        while iteration < self.max_iterations:
            iteration += 1
            
            llm_start = time.time()
            response = await self.provider.chat(
                messages=messages,
                tools=self.tools.get_definitions(),
                model=self.model
            )
            llm_duration_ms = int((time.time() - llm_start) * 1000)
            self._record_usage(
                session_key=session_key,
                model=self.model,
                usage=response.usage,
                duration_ms=llm_duration_ms,
                is_stream=False,
            )
            
            if response.has_tool_calls:
                tool_call_dicts = [
                    {
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.name,
                            "arguments": json.dumps(tc.arguments)
                        }
                    }
                    for tc in response.tool_calls
                ]
                messages = self.context.add_assistant_message(
                    messages, response.content, tool_call_dicts
                )
                
                for tool_call in response.tool_calls:
                    args_str = json.dumps(tool_call.arguments)
                    logger.debug(f"æ­£åœ¨æ‰§è¡Œå·¥å…·ï¼š{tool_call.name}ï¼Œå‚æ•°ï¼š{args_str}")
                    result = await self.tools.execute(tool_call.name, tool_call.arguments)
                    messages = self.context.add_tool_result(
                        messages, tool_call.id, tool_call.name, result
                    )
            else:
                final_content = response.content
                break
        
        if final_content is None:
            final_content = "åå°ä»»åŠ¡å·²å®Œæˆã€?
        
        # ä¿å­˜åˆ°ä¼šè¯ï¼ˆåœ¨å†å²è®°å½•ä¸­æ ‡è®°ä¸ºç³»ç»Ÿæ¶ˆæ¯ï¼‰
        session.add_message("user", f"[ç³»ç»Ÿ: {msg.sender_id}] {msg.content}")
        session.add_message("assistant", final_content)
        self.sessions.save(session)
        
        return OutboundMessage(
            channel=origin_channel,
            chat_id=origin_chat_id,
            content=final_content
        )
    
    async def process_direct(self, content: str, session_key: str = "cli:direct") -> str:
        """
        ç›´æ¥å¤„ç†æ¶ˆæ¯ï¼ˆç”¨äº?CLIï¼‰ã€?
        
        å‚æ•°:
            content: æ¶ˆæ¯å†…å®¹ã€?
            session_key: ä¼šè¯æ ‡è¯†ç¬¦ã€?
        
        è¿”å›:
            Agent çš„å“åº”ã€?
        """
        if ":" in session_key:
            channel, chat_id = session_key.split(":", 1)
        else:
            channel, chat_id = "cli", session_key

        msg = InboundMessage(
            channel=channel,
            sender_id="user",
            chat_id=chat_id,
            content=content
        )
        
        response = await self._process_message(msg)
        return response.content if response else ""

    async def process_direct_stream(
        self,
        content: str,
        session_key: str = "cli:direct",
        on_chunk: Any = None,
        on_trace: Any = None,
        project_info: dict | None = None,
    ) -> str:
        """
        æµå¼ç›´æ¥å¤„ç†æ¶ˆæ¯ï¼Œæ”¯æŒå®æ—¶æ–‡æœ¬è¾“å‡ºå’Œè°ƒç”¨é“¾è·¯è·Ÿè¸ªã€?

        Args:
            content: æ¶ˆæ¯å†…å®¹ã€?
            session_key: ä¼šè¯æ ‡è¯†ç¬¦ã€?
            on_chunk: å¼‚æ­¥å›è°ƒ async def on_chunk(text: str)ï¼Œæ”¶åˆ°æ–‡æœ¬ç‰‡æ®µæ—¶è°ƒç”¨ã€?
            on_trace: å¼‚æ­¥å›è°ƒ async def on_trace(event: dict)ï¼Œè·Ÿè¸ªäº‹ä»¶æ—¶è°ƒç”¨ã€?
            project_info: é¡¹ç›®ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å?id, name, path ç­‰ã€?

        Returns:
            Agent çš„å®Œæ•´å“åº”æ–‡æœ¬ã€?
        """
        if ":" in session_key:
            channel, chat_id = session_key.split(":", 1)
        else:
            channel, chat_id = "cli", session_key

        msg = InboundMessage(
            channel=channel,
            sender_id="user",
            chat_id=chat_id,
            content=content,
        )

        # è®°å½•é¡¹ç›®ä¿¡æ¯
        if project_info:
            logger.info(f"å¤„ç†é¡¹ç›® '{project_info.get('name')}' çš„æ¶ˆæ?(è·¯å¾„: {project_info.get('path')})")

        logger.info(f"æ­£åœ¨å¤„ç†æ¥è‡ª {msg.channel}:{msg.sender_id} çš„æµå¼æ¶ˆæ?)

        request_workspace = self._resolve_request_workspace(project_info)
        request_tools = self._build_request_tools(request_workspace)

        session = self.sessions.get_or_create(msg.session_key)

        # åˆå§‹åŒ–å¹¶è®¾ç½® Harnessï¼ˆé•¿æœŸè¿è¡Œæ¡†æ¶ï¼‰
        from solopreneur.agent.core.harness import LongRunningHarness
        harness = LongRunningHarness(request_workspace)
        self.set_harness(harness)

        # è·å–ä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå·²åˆå§‹åŒ–ï¼?
        harness_context = None
        if harness.is_initialized():
            harness_context = harness.get_session_context()
            logger.info(f"Harness å·²åˆå§‹åŒ–: {harness_context.get('statistics', {})}")

            # effc.md æ¨¡å¼ï¼šä¼šè¯å¼€å§‹æ—¶è‡ªåŠ¨è¿è¡Œå¯åŠ¨æµ‹è¯•
            try:
                startup_tests = harness.run_session_startup_tests()
                if not startup_tests["passed"]:
                    logger.warning(f"ä¼šè¯å¯åŠ¨æµ‹è¯•å¤±è´¥: {startup_tests['summary']}")
                else:
                    logger.info(f"ä¼šè¯å¯åŠ¨æµ‹è¯•é€šè¿‡: {startup_tests['summary']}")
            except Exception as e:
                logger.warning(f"ä¼šè¯å¯åŠ¨æµ‹è¯•æ‰§è¡Œå¼‚å¸¸ï¼ˆä¸é˜»å¡ï¼? {e}")

        # æ›´æ–°å·¥å…·ä¸Šä¸‹æ–?
        message_tool = request_tools.get("message")
        if isinstance(message_tool, MessageTool):
            message_tool.set_context(msg.channel, msg.chat_id)

        spawn_tool = request_tools.get("spawn")
        if isinstance(spawn_tool, SpawnTool):
            spawn_tool.set_context(msg.channel, msg.chat_id)

        messages = self.context.build_messages(
            history=session.get_history(),
            current_message=msg.content,
            project_info=project_info,
        )

        iteration = 0
        start_time = time.time()
        total_tokens = 0
        total_prompt_tokens = 0
        total_completion_tokens = 0
        compacted_count = 0  # å·²å‹ç¼©æ¬¡æ•?

        # ç´¯ç§¯æ‰€æœ‰æµå¼è¾“å‡ºçš„æ–‡æœ¬
        all_streamed: list[str] = []

        async def _emit_trace(event: dict):
            if on_trace:
                await on_trace(event)

        # å°?trace å‘å°„å™¨æ³¨å…¥å­ Agent ç®¡ç†å™¨ï¼Œå½¢æˆç»Ÿä¸€è°ƒç”¨é“?
        self.subagents.set_trace_emitter(_emit_trace)

        async def _track_chunk(text: str):
            all_streamed.append(text)
            if on_chunk:
                await on_chunk(text)

        def _make_result_preview(result: Any, max_chars: int = 800) -> str:
            """ç”Ÿæˆå·¥å…·è¾“å‡ºçš„å¯å±•ç¤ºé¢„è§ˆï¼Œé¿å…?UI è¿‡è½½ã€?""
            try:
                if isinstance(result, str):
                    text = result
                else:
                    text = json.dumps(result, ensure_ascii=False, default=str)
            except Exception:
                text = str(result)

            text = text.strip()
            if len(text) > max_chars:
                return text[:max_chars] + "\n...<truncated>"
            return text

        def _extract_skill_name(tool_name: str, tool_args: dict[str, Any] | None) -> str | None:
            """è¯†åˆ« read_file è¯»å– skill æ–‡ä»¶çš„åœºæ™¯ï¼Œå¹¶æå?skill åç§°ã€?""
            if tool_name != "read_file" or not tool_args:
                return None
            path = str(tool_args.get("path") or "")
            if not path:
                return None

            normalized = path.replace("\\", "/")
            match = re.search(r"/skills/([^/]+)/SKILL\.md$", normalized, re.IGNORECASE)
            if not match:
                return None
            return match.group(1)

        # å‘é€å¼€å§‹äº‹ä»?
        await _emit_trace({
            "event": "start",
            "agent_name": "ä¸»æ§ Agent",
            "model": self.model,
            "session_key": session_key,
            "timestamp": time.time(),
        })

        while iteration < self.max_iterations:
            iteration += 1

            elapsed = time.time() - start_time
            if elapsed > self.max_total_time:
                timeout_msg = f"å¤„ç†è¶…æ—¶ï¼ˆè¶…è¿‡{self.max_total_time // 60}åˆ†é’Ÿï¼‰ï¼Œè¯·ç®€åŒ–æ‚¨çš„è¯·æ±‚æˆ–å¼€å§‹æ–°çš„å¯¹è¯ã€?
                logger.warning(f"Agentå¾ªç¯è¶…æ—¶: {elapsed:.1f}ç§?)
                if on_chunk:
                    await on_chunk(timeout_msg)
                all_streamed.append(timeout_msg)
                break

            # è°ƒç”¨ LLM å‰ï¼šä¸»åŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦å‹ç¼?(78% é˜ˆå€?
            if self.compaction.should_compact(messages, self.max_session_tokens):
                compacted_count += 1
                if compacted_count <= MAX_COMPACTION_ROUNDS:
                    logger.info(
                        f"ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡è¾¾åˆ°é˜ˆå€¼ï¼Œæ‰§è¡Œé¢„é˜²æ€§å‹ç¼?(ç¬¬{compacted_count}æ¬?"
                    )
                    messages = await self._compact_context(messages)
                    total_tokens = 0
                    await _emit_trace({
                        "event": "compaction",
                        "type": "proactive",
                        "round": compacted_count,
                        "timestamp": time.time(),
                    })

            # å‘é€?LLM è°ƒç”¨å¼€å§‹äº‹ä»?
            llm_start = time.time()
            await _emit_trace({
                "event": "llm_start",
                "agent_name": "ä¸»æ§ Agent",
                "iteration": iteration,
                "model": self.model,
                "timestamp": llm_start,
            })

            # ä½¿ç”¨å¸¦é‡è¯•çš„ LLM è°ƒç”¨
            try:
                response, call_success = await self._call_llm_with_retry(
                    messages=messages,
                    tools=request_tools.get_definitions(),
                    on_chunk=_track_chunk,
                    use_stream=True,
                )
                llm_duration_ms = int((time.time() - llm_start) * 1000)
                self._record_usage(
                    session_key=msg.session_key,
                    model=self.model,
                    usage=response.usage,
                    duration_ms=llm_duration_ms,
                    is_stream=True,
                )
            except Exception as e:
                # é‡è¯•è€—å°½åçš„é”™è¯¯å¤„ç†
                error_msg = f"LLM è°ƒç”¨å¤±è´¥: {type(e).__name__}: {e}"
                logger.error(error_msg)
                if on_chunk:
                    await on_chunk(f"\n\n**[é”™è¯¯]** {error_msg}")
                all_streamed.append(error_msg)
                break

            llm_end = time.time()

            # è·å– token æ•°æ®ï¼Œå¦‚æ?provider æ²¡æœ‰è¿”å›åˆ™ä¼°ç®?
            if response.usage and any(response.usage.values()):
                # Provider è¿”å›äº?usage æ•°æ®
                prompt_tokens = response.usage.get("prompt_tokens", 0)
                completion_tokens = response.usage.get("completion_tokens", 0)
                step_tokens = response.usage.get("total_tokens", 0)
            else:
                # Provider æ²¡æœ‰è¿”å› usage æ•°æ®ï¼Œä½¿ç”¨ç®€å•ä¼°ç®?
                # å‡è®¾ï¼šä¸­æ–‡çº¦ 1.5 token/å­—ï¼Œè‹±æ–‡çº?0.25 token/å­?
                # è¿™é‡Œä½¿ç”¨ç²—ç•¥ä¼°ç®—ï¼šå­—ç¬¦æ•° / 2
                total_chars = sum(len(msg.get("content", "")) for msg in messages)
                output_chars = len("".join(all_streamed)) if all_streamed else 0
                prompt_tokens = total_chars // 2
                completion_tokens = output_chars // 2
                step_tokens = prompt_tokens + completion_tokens
                logger.warning(
                    f"LLM provider did not return usage data, estimating tokens: "
                    f"prompt={prompt_tokens}, completion={completion_tokens}, total={step_tokens}"
                )

            total_tokens += step_tokens
            total_prompt_tokens += prompt_tokens
            total_completion_tokens += completion_tokens

            # è°ƒè¯•æ—¥å¿—
            logger.info(f"LLM usage data: {response.usage}, tokens: prompt={prompt_tokens}, completion={completion_tokens}, total={step_tokens}")

            # å‘é€?LLM è°ƒç”¨ç»“æŸäº‹ä»¶
            await _emit_trace({
                "event": "llm_end",
                "agent_name": "ä¸»æ§ Agent",
                "iteration": iteration,
                "model": self.model,
                "duration_ms": round((llm_end - llm_start) * 1000),
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": step_tokens,
                "cumulative_tokens": total_tokens,
                "has_tool_calls": response.has_tool_calls,
                "timestamp": llm_end,
            })

            if total_tokens > self.max_session_tokens:
                compacted_count += 1
                if compacted_count >= MAX_COMPACTION_ROUNDS:
                    warn_msg = f"Tokenæ¶ˆè€—è¿‡å¤šï¼ˆ{total_tokens}ï¼‰ï¼Œå·²å¤šæ¬¡å‹ç¼©ä¸Šä¸‹æ–‡ä»æ— æ³•æ§åˆ¶ï¼Œè¯·å¼€å§‹æ–°çš„å¯¹è¯ã€?
                    logger.warning(f"Tokenæ¶ˆè€—è¶…é™ä¸”å‹ç¼©æ¬¡æ•°ç”¨å°½: {total_tokens}")
                    if on_chunk:
                        await on_chunk(warn_msg)
                    all_streamed.append(warn_msg)
                    break
                logger.info(
                    f"Tokenç´¯è®¡ {total_tokens}/{self.max_session_tokens} è¶…é™ï¼?
                    f"æ‰§è¡Œä¸Šä¸‹æ–‡å‹ç¼?(ç¬¬{compacted_count}æ¬?"
                )
                messages = await self._compact_context(messages)
                total_tokens = 0  # å‹ç¼©åé‡ç½®è®¡æ•?
                await _emit_trace({
                    "event": "compaction",
                    "type": "reactive",
                    "round": compacted_count,
                    "timestamp": time.time(),
                })

            if response.has_tool_calls:
                tool_call_dicts = [
                    {
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.name,
                            "arguments": json.dumps(tc.arguments),
                        },
                    }
                    for tc in response.tool_calls
                ]
                messages = self.context.add_assistant_message(
                    messages, response.content, tool_call_dicts
                )

                for tool_call in response.tool_calls:
                    tool_args = dict(tool_call.arguments or {})
                    if tool_call.name == "run_workflow" and project_info:
                        if not tool_args.get("project_dir") and project_info.get("path"):
                            tool_args["project_dir"] = str(project_info.get("path"))
                        if not tool_args.get("project_name") and project_info.get("name"):
                            tool_args["project_name"] = str(project_info.get("name"))

                    delegate_agent = None
                    if tool_call.name == "delegate":
                        delegate_agent = str(tool_args.get("agent", ""))

                    skill_name = _extract_skill_name(tool_call.name, tool_args)
                    if skill_name:
                        await _emit_trace({
                            "event": "skill_start",
                            "agent_name": "ä¸»æ§ Agent",
                            "iteration": iteration,
                            "skill_name": skill_name,
                            "tool_name": tool_call.name,
                            "tool_args": tool_args,
                            "timestamp": time.time(),
                        })

                    tool_start = time.time()
                    await _emit_trace({
                        "event": "tool_start",
                        "agent_name": "ä¸»æ§ Agent",
                        "iteration": iteration,
                        "tool_name": tool_call.name,
                        "delegate_agent": delegate_agent,
                        "tool_args": tool_args,
                        "timestamp": tool_start,
                    })

                    logger.debug(
                        f"æ­£åœ¨æ‰§è¡Œå·¥å…·ï¼š{tool_call.name}ï¼?
                        f"å‚æ•°ï¼š{json.dumps(tool_args)}"
                    )
                    result = await request_tools.execute(
                        tool_call.name, tool_args
                    )
                    if tool_call.name in {"run_workflow", "delegate", "delegate_parallel", "delegate_auto"}:
                        logger.info(f"è¯·æ±‚ä½œç”¨åŸŸå·¥ä½œç›®å½? {request_workspace}")
                    tool_end = time.time()
                    await _emit_trace({
                        "event": "tool_end",
                        "agent_name": "ä¸»æ§ Agent",
                        "iteration": iteration,
                        "tool_name": tool_call.name,
                        "delegate_agent": delegate_agent,
                        "duration_ms": round((tool_end - tool_start) * 1000),
                        "result_length": len(str(result)),
                        "result_preview": _make_result_preview(result),
                        "timestamp": tool_end,
                    })

                    if skill_name:
                        await _emit_trace({
                            "event": "skill_end",
                            "agent_name": "ä¸»æ§ Agent",
                            "iteration": iteration,
                            "skill_name": skill_name,
                            "duration_ms": round((tool_end - tool_start) * 1000),
                            "result_length": len(str(result)),
                            "result_preview": _make_result_preview(result),
                            "timestamp": tool_end,
                        })

                    messages = self.context.add_tool_result(
                        messages, tool_call.id, tool_call.name, result
                    )

                # å·¥å…·è°ƒç”¨åæ‰§è¡Œå¾®å‹ç¼©ï¼ˆå¤§å‹è¾“å‡ºè½ç›˜ï¼‰
                messages = self.compaction.microcompact(messages)
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼ŒéªŒè¯ä»»åŠ¡æ˜¯å¦çœŸæ­£å®Œæˆ?
                should_continue = False
                continuation_prompt = ""

                # 1. æ£€æŸ¥æœ€å°è¿­ä»£æ¬¡æ•?
                force_continue, reason = self.validator.should_force_continue(iteration)
                if force_continue:
                    should_continue = True
                    continuation_prompt = f"ä»»åŠ¡åˆšå¼€å§‹ï¼Œè¯·ç»§ç»­è°ƒç”¨å·¥å…·å·¥ä½œã€‚\nåŸå› : {reason}"
                    logger.info(f"[Stream] è¿­ä»£æ¬¡æ•°ä¸è¶³ ({iteration}/{self.validator_config.min_iterations})ï¼Œå¼ºåˆ¶ç»§ç»?)

                # 2. ä»»åŠ¡å®ŒæˆéªŒè¯ï¼ˆä»…å½“æœ€å°è¿­ä»£æ»¡è¶³æ—¶ï¼?
                if not should_continue and self.validator.can_send_continuation_prompt():
                    # æ”¶é›†å½“å‰å·²æµå¼è¾“å‡ºçš„å†…å®¹ç”¨äºéªŒè¯
                    current_content = "".join(all_streamed)
                    validation_result = await self.validator.validate(current_content or response.content or "")
                    if not validation_result.is_complete:
                        should_continue = True
                        continuation_prompt = validation_result.get_continuation_prompt()
                        self.validator.increment_continuation_count()
                        logger.info(f"[Stream] ä»»åŠ¡å®ŒæˆéªŒè¯æœªé€šè¿‡: {validation_result.reasons}")

                if should_continue:
                    # æ³¨å…¥ç»§ç»­æç¤ºï¼Œå¼ºåˆ?LLM ç»§ç»­å·¥ä½œ
                    messages.append({
                        "role": "user",
                        "content": continuation_prompt
                    })
                    # é€šçŸ¥ UI ä»»åŠ¡æœªå®Œæˆï¼Œç»§ç»­å¤„ç†
                    if on_chunk:
                        await on_chunk(f"\n\n---\n**[ç³»ç»Ÿ] ä»»åŠ¡æœªå®Œæˆï¼Œç»§ç»­æ‰§è¡Œ...**\n\n")
                    continue

                # éªŒè¯é€šè¿‡ï¼Œå¤„ç†å®Œæˆ?
                break

        final_content = "".join(all_streamed) if all_streamed else "æˆ‘å·²å¤„ç†å®Œæ¯•ï¼Œä½†æ²¡æœ‰ç”Ÿæˆä»»ä½•å“åº”ã€?

        total_elapsed = time.time() - start_time
        # å‘é€ç»“æŸäº‹ä»?
        await _emit_trace({
            "event": "end",
            "agent_name": "ä¸»æ§ Agent",
            "total_iterations": iteration,
            "total_tokens": total_tokens,
            "prompt_tokens": total_prompt_tokens,
            "completion_tokens": total_completion_tokens,
            "total_duration_ms": round(total_elapsed * 1000),
            "model": self.model,
            "timestamp": time.time(),
        })

        # ä¿å­˜åˆ°ä¼šè¯?
        session.add_message("user", msg.content)
        session.add_message("assistant", final_content)
        self.sessions.save(session)

        return final_content
