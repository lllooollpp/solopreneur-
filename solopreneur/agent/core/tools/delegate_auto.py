"""
Delegate Auto å·¥å…· - è‡ªåŠ¨ä¾èµ–åˆ†æåä¸²å¹¶è¡Œæ··åˆæ‰§è¡Œã€?
"""

from __future__ import annotations

import json
import re
from collections import defaultdict, deque
from typing import TYPE_CHECKING, Any

from solopreneur.agent.core.tools.base import Tool

if TYPE_CHECKING:
    from solopreneur.agent.core.subagent import SubagentManager
    from solopreneur.agent.definitions.manager import AgentManager


class DelegateAutoTool(Tool):
    """æ ¹æ®ä»»åŠ¡ä¾èµ–è‡ªåŠ¨é€‰æ‹©ä¸²è¡Œæˆ–å¹¶è¡Œå§”æ´¾ã€?""

    _PATH_RE = re.compile(r"[A-Za-z0-9_./\\-]+\.(?:py|ts|tsx|js|jsx|vue|java|kt|go|rs|sql|md|yaml|yml|json)")

    def __init__(self, manager: "SubagentManager", agent_manager: "AgentManager"):
        self._manager = manager
        self._agent_manager = agent_manager

    @property
    def name(self) -> str:
        return "delegate_auto"

    @property
    def description(self) -> str:
        return (
            "è‡ªåŠ¨åˆ†æä»»åŠ¡ä¾èµ–å¹¶æ‰§è¡Œæ··åˆè°ƒåº¦ï¼š"
            "ç‹¬ç«‹ä»»åŠ¡å¹¶è¡Œï¼Œå­˜åœ¨ä¾èµ–çš„ä»»åŠ¡ä¸²è¡Œã€?
        )

    @property
    def parameters(self) -> dict[str, Any]:
        agent_names = self._agent_manager.get_agent_names()
        return {
            "type": "object",
            "properties": {
                "jobs": {
                    "type": "array",
                    "description": "ä»»åŠ¡åˆ—è¡¨ã€‚å¯é€?depends_on æŒ‡æ˜ä¾èµ–ï¼›ä¸æä¾›æ—¶è‡ªåŠ¨æ¨æ–­ã€?,
                    "items": {
                        "type": "object",
                        "properties": {
                            "id": {"type": "string", "description": "å¯é€‰ä»»åŠ¡ID"},
                            "agent": {"type": "string", "enum": agent_names, "description": "Agentåç§°"},
                            "task": {"type": "string", "description": "ä»»åŠ¡æè¿°"},
                            "context": {"type": "string", "description": "å¯é€‰ä¸Šä¸‹æ–‡"},
                            "project_dir": {"type": "string", "description": "å¯é€‰é¡¹ç›®ç›®å½?},
                            "depends_on": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "å¯é€‰ä¾èµ–ä»»åŠ¡IDåˆ—è¡¨",
                            },
                        },
                        "required": ["agent", "task"],
                    },
                },
                "max_parallel": {
                    "type": "integer",
                    "minimum": 1,
                    "maximum": 8,
                    "description": "æœ€å¤§å¹¶å‘åº¦ï¼Œé»˜è®?",
                },
            },
            "required": [],
        }

    async def execute(self, jobs: list[dict[str, Any]] | None = None, max_parallel: int = 3, **kwargs: Any) -> str:
        if not isinstance(jobs, list) or not jobs:
            raw = kwargs.get("raw")
            parsed_jobs, parsed_parallel = self._parse_jobs_from_raw(raw)
            if parsed_parallel is not None:
                max_parallel = parsed_parallel
            jobs = parsed_jobs

        if not jobs:
            return (
                "é”™è¯¯: jobs ä¸èƒ½ä¸ºç©ºã€‚è¯·ä¼ å…¥ jobs æ•°ç»„ï¼?
                "æˆ–æä¾›å¯è§£æçš?raw JSONï¼ˆå« agent/task åˆ—è¡¨ï¼‰ã€?
            )

        normalized = self._normalize_jobs(jobs)
        self._apply_auto_dependencies(normalized)
        layers, cycle = self._topo_layers(normalized)

        if cycle:
            # å›é€€åˆ°è¾“å…¥é¡ºåºä¸²è¡Œï¼Œç¡®ä¿å¯æ‰§è¡?
            layers = [[j["id"] for j in normalized]]

        id_to_job = {j["id"]: j for j in normalized}
        id_to_result: dict[str, str] = {}
        id_to_ok: dict[str, bool] = {}

        for layer in layers:
            run_jobs = []
            layer_ids = []
            for job_id in layer:
                job = id_to_job[job_id]
                dep_ctx = []
                for dep_id in job["depends_on"]:
                    if dep_id in id_to_result:
                        dep_ctx.append(f"[ä¾èµ– {dep_id}]\n{id_to_result[dep_id]}")
                merged_context = (job.get("context") or "").strip()
                if dep_ctx:
                    merged_context = (merged_context + "\n\n" + "\n\n".join(dep_ctx)).strip()

                run_jobs.append(
                    {
                        "agent": job["agent"],
                        "task": job["task"],
                        "context": merged_context,
                        "project_dir": job.get("project_dir") or "",
                    }
                )
                layer_ids.append(job_id)

            if len(run_jobs) == 1:
                only_id = layer_ids[0]
                only = run_jobs[0]
                agent_def = self._agent_manager.get_agent(only["agent"])
                if not agent_def:
                    id_to_ok[only_id] = False
                    id_to_result[only_id] = f"é”™è¯¯: unknown agent {only['agent']}"
                else:
                    try:
                        out = await self._manager.run_with_agent(
                            agent_def=agent_def,
                            agent_manager=self._agent_manager,
                            task=only["task"],
                            context=only["context"],
                            project_dir=only["project_dir"],
                        )
                        id_to_ok[only_id] = True
                        id_to_result[only_id] = out
                    except Exception as e:
                        id_to_ok[only_id] = False
                        id_to_result[only_id] = f"é”™è¯¯: {e}"
            else:
                outs = await self._manager.run_with_agents_parallel(
                    agent_manager=self._agent_manager,
                    jobs=run_jobs,
                    max_parallel=max_parallel,
                )
                for i, out in enumerate(outs):
                    jid = layer_ids[i]
                    id_to_ok[jid] = bool(out.get("ok"))
                    id_to_result[jid] = out.get("result") if out.get("ok") else f"é”™è¯¯: {out.get('error', 'unknown')}"

        ok_count = sum(1 for jid in id_to_ok if id_to_ok[jid])
        fail_count = len(normalized) - ok_count

        lines = [
            f"è‡ªåŠ¨è°ƒåº¦å®Œæˆï¼šæˆåŠ?{ok_count} / å¤±è´¥ {fail_count}",
            f"è°ƒåº¦å±‚æ•°: {len(layers)}" + (" (æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–ï¼Œå·²å›é€€ä¸²è¡Œ)" if cycle else ""),
            "",
            "## è°ƒåº¦è®¡åˆ’",
        ]

        for idx, layer in enumerate(layers, 1):
            labels = [f"{jid}({id_to_job[jid]['agent']})" for jid in layer]
            lines.append(f"- ç¬¬{idx}å±?{'å¹¶è¡Œ' if len(layer) > 1 else 'ä¸²è¡Œ'}: " + ", ".join(labels))

        lines.append("")
        lines.append("## æ‰§è¡Œç»“æœ")
        for job in normalized:
            jid = job["id"]
            status = "âœ? if id_to_ok.get(jid) else "â?
            lines.append(f"### {status} {jid} Â· {job['agent']}")
            lines.append(id_to_result.get(jid, ""))
            lines.append("")

        return "\n".join(lines)

    def _normalize_jobs(self, jobs: list[dict[str, Any]]) -> list[dict[str, Any]]:
        normalized: list[dict[str, Any]] = []
        used_ids: set[str] = set()
        for i, j in enumerate(jobs, 1):
            jid = str(j.get("id") or f"job_{i}").strip()
            if not jid or jid in used_ids:
                jid = f"job_{i}"
            used_ids.add(jid)

            task = str(j.get("task") or "").strip()
            agent = str(j.get("agent") or "").strip()
            depends = [str(x).strip() for x in (j.get("depends_on") or []) if str(x).strip()]

            normalized.append(
                {
                    "id": jid,
                    "agent": agent,
                    "task": task,
                    "context": str(j.get("context") or ""),
                    "project_dir": str(j.get("project_dir") or ""),
                    "depends_on": depends,
                    "class": self._classify(task),
                    "paths": set(self._extract_paths(task)),
                }
            )
        return normalized

    def _apply_auto_dependencies(self, jobs: list[dict[str, Any]]) -> None:
        # è‹¥å·²ç»æ˜¾å¼å£°æ˜ä¾èµ–åˆ™ä¼˜å…ˆä½¿ç”¨
        has_explicit = any(j["depends_on"] for j in jobs)
        if has_explicit:
            return

        for i, cur in enumerate(jobs):
            # å®¡æŸ¥/æµ‹è¯•/å‘å¸ƒç±»é»˜è®¤ä¾èµ–å‰åºå®ç°ä»»åŠ?
            if cur["class"] in {"review", "test", "release", "integration"}:
                cur["depends_on"].extend(j["id"] for j in jobs[:i])
                continue

            # åŒæ–‡ä»¶ç›®æ ‡å†²çªä»»åŠ¡ä¸²è¡Œï¼ˆåè€…ä¾èµ–å‰è€…ï¼‰
            for prev in jobs[:i]:
                if cur["paths"] and prev["paths"] and (cur["paths"] & prev["paths"]):
                    cur["depends_on"].append(prev["id"])

        # å»é‡ä¿æŒé¡ºåº
        for j in jobs:
            seen: set[str] = set()
            dedup = []
            for dep in j["depends_on"]:
                if dep not in seen:
                    dedup.append(dep)
                    seen.add(dep)
            j["depends_on"] = dedup

    def _topo_layers(self, jobs: list[dict[str, Any]]) -> tuple[list[list[str]], bool]:
        ids = {j["id"] for j in jobs}
        indeg = defaultdict(int)
        edges = defaultdict(list)

        for j in jobs:
            indeg[j["id"]] = indeg[j["id"]]
            for dep in j["depends_on"]:
                if dep not in ids:
                    continue
                edges[dep].append(j["id"])
                indeg[j["id"]] += 1

        q = deque([jid for jid in ids if indeg[jid] == 0])
        layers: list[list[str]] = []
        visited = 0

        while q:
            layer_size = len(q)
            layer: list[str] = []
            for _ in range(layer_size):
                cur = q.popleft()
                visited += 1
                layer.append(cur)
                for nxt in edges[cur]:
                    indeg[nxt] -= 1
                    if indeg[nxt] == 0:
                        q.append(nxt)
            layers.append(layer)

        has_cycle = visited != len(ids)

        # ç¨³å®šé¡ºåºï¼šæŒ‰è¾“å…¥é¡ºåºé‡æ’æ¯å±‚
        input_order = {j["id"]: i for i, j in enumerate(jobs)}
        for layer in layers:
            layer.sort(key=lambda x: input_order.get(x, 0))

        return layers, has_cycle

    def _extract_paths(self, text: str) -> list[str]:
        return [p.replace("\\", "/") for p in self._PATH_RE.findall(text or "")]

    def _parse_jobs_from_raw(self, raw: Any) -> tuple[list[dict[str, Any]], int | None]:
        """ä»ç•¸å½?raw å­—ç¬¦ä¸²ä¸­å°½é‡æ¢å¤ jobs ä¸?max_parallelã€?""
        if not isinstance(raw, str) or not raw.strip():
            return [], None

        text = raw.strip()

        # å?markdown åŒ…è£¹
        text = re.sub(r"^```(?:json)?\s*", "", text)
        text = re.sub(r"\s*```$", "", text)

        parsed_parallel: int | None = None
        m = re.search(r'"max_parallel"\s*:\s*(\d+)', text)
        if m:
            try:
                parsed_parallel = int(m.group(1))
            except Exception:
                parsed_parallel = None

        # å…ˆå°è¯•æ ‡å‡?JSON
        try:
            obj = json.loads(text)
            if isinstance(obj, dict):
                jobs = obj.get("jobs")
                if isinstance(jobs, list):
                    if isinstance(obj.get("max_parallel"), int):
                        parsed_parallel = obj["max_parallel"]
                    return jobs, parsed_parallel
                if "agent" in obj and "task" in obj:
                    return [obj], parsed_parallel
            elif isinstance(obj, list):
                return [j for j in obj if isinstance(j, dict)], parsed_parallel
        except Exception:
            pass

        # å›é€€ï¼šæŠ½å–å¤šä¸ªé¡¶å±?{ ... } å¯¹è±¡
        jobs: list[dict[str, Any]] = []
        for frag in self._extract_json_objects(text):
            try:
                item = json.loads(frag)
            except Exception:
                continue
            if not isinstance(item, dict):
                continue

            # å…¼å®¹ç¬¬ä¸€æ®µé”™è¯¯æ ¼å¼ï¼š{"jobs":"architect", "agent":"architect", "task":"..."}
            if "agent" in item and "task" in item:
                if isinstance(item.get("jobs"), str) and not item.get("id"):
                    item["id"] = item["jobs"]
                jobs.append(item)

        return jobs, parsed_parallel

    def _extract_json_objects(self, text: str) -> list[str]:
        """ä»æ–‡æœ¬ä¸­æå–é¡¶å±‚ JSON å¯¹è±¡ç‰‡æ®µã€?""
        parts: list[str] = []
        depth = 0
        start = -1
        in_str = False
        esc = False

        for i, ch in enumerate(text):
            if in_str:
                if esc:
                    esc = False
                elif ch == "\\":
                    esc = True
                elif ch == '"':
                    in_str = False
                continue

            if ch == '"':
                in_str = True
                continue

            if ch == '{':
                if depth == 0:
                    start = i
                depth += 1
            elif ch == '}':
                if depth > 0:
                    depth -= 1
                    if depth == 0 and start != -1:
                        parts.append(text[start:i + 1])
                        start = -1

        return parts

    def _classify(self, task: str) -> str:
        t = (task or "").lower()
        if any(k in t for k in ["test", "æµ‹è¯•", "å•æµ‹", "é›†æˆæµ‹è¯•"]):
            return "test"
        if any(k in t for k in ["review", "å®¡æŸ¥", "code review", "è¯„å®¡"]):
            return "review"
        if any(k in t for k in ["release", "deploy", "ä¸Šçº¿", "å‘å¸ƒ"]):
            return "release"
        if any(k in t for k in ["è”è°ƒ", "integration", "é›†æˆ"]):
            return "integration"
        return "implementation"
